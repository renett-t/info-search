{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f0d077-9188-4110-ae74-9cb82c324689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from glob import glob\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1abeb2c-0374-4d49-a243-a7ce01284772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian websites: 195\n"
     ]
    }
   ],
   "source": [
    "RU_WEBS_DIR = \"./../a-crawling/crawling/crawling/spiders/pages/ru\"\n",
    "ru_webs_pl = glob(f\"{RU_WEBS_DIR}/**.html\")\n",
    "\n",
    "print(f\"Russian websites: {len(ru_webs_pl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcac1247-d348-4b70-b32b-5b6107337c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English websites: 565\n"
     ]
    }
   ],
   "source": [
    "EN_WEBS_DIR = \"./../a-crawling/crawling/crawling/spiders/pages/en\"\n",
    "en_webs_pl = glob(f\"{EN_WEBS_DIR}/**.html\")\n",
    "\n",
    "print(f\"English websites: {len(en_webs_pl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e541d9e-6e7c-437a-bee1-b86a73d14b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "RU_LEMMAS_FILE = \"./../b-tokens/ru_lemmas.txt\"\n",
    "RU_TOKENS_FILE = \"./../b-tokens/ru_tokens.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711989db-71d0-4d3c-95a0-2cf68a538889",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_LEMMAS_FILE = \"./../b-tokens/en_lemmas.txt\"\n",
    "EN_TOKENS_FILE = \"./../b-tokens/en_tokens.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e96f60-3013-4f3d-a744-42c91f17f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfForToken:\n",
    "    def __init__(self, token, tf):\n",
    "        self.token = token\n",
    "        self.tf = tf\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TfForToken(token = '{self.token}', tf = {self.tf})\"\n",
    "\n",
    "\n",
    "class FileTfIdfModel:\n",
    "    def __init__(self, file_idx: int, tfs: List[TfForToken]):\n",
    "        self.file_idx = file_idx\n",
    "        self.tfs = tfs\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"FileTfIdfModel(file_idx = '{self.file_idx}', tfs = {self.tfs},)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf7d2e9-77d0-47ce-891f-e55c270eaffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r.tyapkina/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/r.tyapkina/Library/Python/3.9/lib/python/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/r.tyapkina/Library/Python/3.9/lib/python/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import html2text\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "\n",
    "\n",
    "h2t = html2text.HTML2Text()\n",
    "h2t.ignore_links = True\n",
    "\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "def get_tokens(text, lang):\n",
    "    if (lang == \"RU\"):\n",
    "        doc = nlp_ru(text)\n",
    "    else:\n",
    "        doc = nlp_en(text)\n",
    "    \n",
    "    return [w for w in doc if (w.is_alpha and not w.is_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4047e2c3-3a00-4a1c-a106-8cf914c2fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "def calculate_idf_map(idf_files_map: {}, total_files_count: int) -> Dict:\n",
    "    # token (string) - key, idf (double) - value\n",
    "    idf_map = {}\n",
    "    \n",
    "    for token in idf_files_map.keys():\n",
    "        files_appears_in = idf_files_map[token]\n",
    "        files_appears_in_count = len(files_appears_in)\n",
    "    \n",
    "        idf = log10(total_files_count / files_appears_in_count)\n",
    "        idf_map[token] = idf\n",
    "\n",
    "    return idf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d15be4-030a-4af0-8394-2e080fb8dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_tf_idf_results(tfs_per_file_list: List[FileTfIdfModel], idf_map: Dict, output_dir: str):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for tfs_for_file in tfs_per_file_list:\n",
    "        \n",
    "        file_name = output_dir + str(tfs_for_file.file_idx) + \".txt\"\n",
    "        \n",
    "        with open(file_name, \"w\") as file:\n",
    "            for token_entry in tfs_for_file.tfs:\n",
    "                token = token_entry.token\n",
    "                \n",
    "                tf = token_entry.tf\n",
    "                idf = idf_map[token]\n",
    "                tf_idf = tf * idf\n",
    "                \n",
    "                file.write(f\"{token} {tf} {tf_idf}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6f7b5f6-c227-4d14-8df7-f6581d524fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF for TOKENS\n",
    "\n",
    "def create_tf_idf_for_tokens(lang: str, webs_pl: list, input_tokens_file: str, output_dir: str):\n",
    "    \n",
    "    total_files_count = len(webs_pl)\n",
    "    # token (string) - key, files_appears_in (set of int) - value\n",
    "    idf_files_map = {}\n",
    "    # list of FileTfIdfModel\n",
    "    tfs_per_file_list = []\n",
    "\n",
    "    for pl in webs_pl:\n",
    "        # list of TfForToken\n",
    "        tfs_list = []\n",
    "    \n",
    "        file_idx_match = re.search(r'(.+)/(\\d+)-.*\\.html$', pl)\n",
    "        file_idx = int(file_idx_match.group(2))\n",
    "        \n",
    "        with open(pl, \"r\") as file:\n",
    "            file_content = file.read()\n",
    "            file_text = h2t.handle(file_content)\n",
    "            \n",
    "            file_tokens = get_tokens(file_text, lang)\n",
    "            \n",
    "            unique_tokens = {el.text.lower() for el in file_tokens}\n",
    "    \n",
    "            total_tokens_count = len(file_tokens)\n",
    "            print(f'{file_idx} file has {len(unique_tokens)} of unique tokens')\n",
    "            \n",
    "            for u_token in unique_tokens:\n",
    "                # сколько раз встречается токен u_token в тексте данного документа file\n",
    "                token_count = sum(1 for t in file_tokens if t.text == u_token)\n",
    "    \n",
    "                # term frequency для u_token для данного документа\n",
    "                tf = token_count / total_tokens_count\n",
    "    \n",
    "                # добавляем u_token в idf_map\n",
    "                if (u_token not in idf_files_map.keys()):\n",
    "                    idf_files_map[u_token] = set()\n",
    "                idf_files_map[u_token].add(file_idx)\n",
    "    \n",
    "                tfs_list.append(TfForToken(u_token, tf))\n",
    "    \n",
    "        file_tfs = FileTfIdfModel(file_idx, tfs_list)\n",
    "        tfs_per_file_list.append(file_tfs)\n",
    "\n",
    "    print()\n",
    "    print(f'IDF map contains {len(idf_files_map.keys())} entries')\n",
    "    print(f'Files processed = {len(tfs_per_file_list)}')\n",
    "\n",
    "    idf_map = calculate_idf_map(idf_files_map=idf_files_map, total_files_count=total_files_count)\n",
    "\n",
    "    write_tf_idf_results(tfs_per_file_list=tfs_per_file_list, idf_map=idf_map, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40121f32-8200-4df8-8c4a-71a63f22ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 file has 422 of unique tokens\n",
      "191 file has 880 of unique tokens\n",
      "87 file has 205 of unique tokens\n",
      "174 file has 434 of unique tokens\n",
      "124 file has 245 of unique tokens\n",
      "95 file has 95 of unique tokens\n",
      "169 file has 767 of unique tokens\n",
      "80 file has 81 of unique tokens\n",
      "94 file has 81 of unique tokens\n",
      "49 file has 245 of unique tokens\n",
      "118 file has 342 of unique tokens\n",
      "69 file has 206 of unique tokens\n",
      "194 file has 1392 of unique tokens\n",
      "139 file has 156 of unique tokens\n",
      "68 file has 173 of unique tokens\n",
      "71 file has 49 of unique tokens\n",
      "89 file has 131 of unique tokens\n",
      "184 file has 1702 of unique tokens\n",
      "173 file has 525 of unique tokens\n",
      "23 file has 172 of unique tokens\n",
      "51 file has 86 of unique tokens\n",
      "99 file has 81 of unique tokens\n",
      "45 file has 69 of unique tokens\n",
      "128 file has 124 of unique tokens\n",
      "106 file has 494 of unique tokens\n",
      "33 file has 688 of unique tokens\n",
      "136 file has 149 of unique tokens\n",
      "93 file has 85 of unique tokens\n",
      "141 file has 146 of unique tokens\n",
      "13 file has 163 of unique tokens\n",
      "102 file has 264 of unique tokens\n",
      "10 file has 627 of unique tokens\n",
      "116 file has 421 of unique tokens\n",
      "16 file has 512 of unique tokens\n",
      "160 file has 613 of unique tokens\n",
      "183 file has 403 of unique tokens\n",
      "105 file has 81 of unique tokens\n",
      "172 file has 434 of unique tokens\n",
      "112 file has 298 of unique tokens\n",
      "179 file has 314 of unique tokens\n",
      "91 file has 136 of unique tokens\n",
      "58 file has 68 of unique tokens\n",
      "148 file has 418 of unique tokens\n",
      "88 file has 122 of unique tokens\n",
      "178 file has 323 of unique tokens\n",
      "70 file has 188 of unique tokens\n",
      "22 file has 326 of unique tokens\n",
      "190 file has 384 of unique tokens\n",
      "11 file has 368 of unique tokens\n",
      "111 file has 491 of unique tokens\n",
      "109 file has 331 of unique tokens\n",
      "133 file has 122 of unique tokens\n",
      "165 file has 468 of unique tokens\n",
      "162 file has 478 of unique tokens\n",
      "42 file has 283 of unique tokens\n",
      "140 file has 134 of unique tokens\n",
      "187 file has 418 of unique tokens\n",
      "15 file has 554 of unique tokens\n",
      "50 file has 257 of unique tokens\n",
      "75 file has 260 of unique tokens\n",
      "4 file has 598 of unique tokens\n",
      "86 file has 530 of unique tokens\n",
      "170 file has 653 of unique tokens\n",
      "14 file has 494 of unique tokens\n",
      "108 file has 390 of unique tokens\n",
      "166 file has 125 of unique tokens\n",
      "97 file has 232 of unique tokens\n",
      "1 file has 384 of unique tokens\n",
      "142 file has 481 of unique tokens\n",
      "158 file has 595 of unique tokens\n",
      "171 file has 879 of unique tokens\n",
      "21 file has 91 of unique tokens\n",
      "77 file has 306 of unique tokens\n",
      "175 file has 755 of unique tokens\n",
      "122 file has 209 of unique tokens\n",
      "152 file has 212 of unique tokens\n",
      "92 file has 69 of unique tokens\n",
      "35 file has 199 of unique tokens\n",
      "54 file has 295 of unique tokens\n",
      "103 file has 514 of unique tokens\n",
      "110 file has 81 of unique tokens\n",
      "150 file has 510 of unique tokens\n",
      "119 file has 436 of unique tokens\n",
      "36 file has 595 of unique tokens\n",
      "147 file has 372 of unique tokens\n",
      "129 file has 102 of unique tokens\n",
      "163 file has 545 of unique tokens\n",
      "186 file has 340 of unique tokens\n",
      "146 file has 81 of unique tokens\n",
      "96 file has 470 of unique tokens\n",
      "176 file has 306 of unique tokens\n",
      "20 file has 306 of unique tokens\n",
      "66 file has 266 of unique tokens\n",
      "72 file has 49 of unique tokens\n",
      "192 file has 426 of unique tokens\n",
      "61 file has 328 of unique tokens\n",
      "46 file has 66 of unique tokens\n",
      "168 file has 185 of unique tokens\n",
      "17 file has 147 of unique tokens\n",
      "78 file has 254 of unique tokens\n",
      "193 file has 470 of unique tokens\n",
      "104 file has 443 of unique tokens\n",
      "85 file has 328 of unique tokens\n",
      "53 file has 430 of unique tokens\n",
      "123 file has 184 of unique tokens\n",
      "83 file has 163 of unique tokens\n",
      "157 file has 100 of unique tokens\n",
      "12 file has 300 of unique tokens\n",
      "3 file has 494 of unique tokens\n",
      "117 file has 270 of unique tokens\n",
      "167 file has 380 of unique tokens\n",
      "145 file has 352 of unique tokens\n",
      "25 file has 956 of unique tokens\n",
      "189 file has 187 of unique tokens\n",
      "64 file has 126 of unique tokens\n",
      "98 file has 556 of unique tokens\n",
      "100 file has 67 of unique tokens\n",
      "29 file has 69 of unique tokens\n",
      "57 file has 338 of unique tokens\n",
      "135 file has 139 of unique tokens\n",
      "34 file has 549 of unique tokens\n",
      "125 file has 370 of unique tokens\n",
      "151 file has 188 of unique tokens\n",
      "47 file has 432 of unique tokens\n",
      "18 file has 291 of unique tokens\n",
      "63 file has 17 of unique tokens\n",
      "26 file has 452 of unique tokens\n",
      "32 file has 470 of unique tokens\n",
      "62 file has 123 of unique tokens\n",
      "159 file has 669 of unique tokens\n",
      "195 file has 82 of unique tokens\n",
      "107 file has 81 of unique tokens\n",
      "82 file has 554 of unique tokens\n",
      "37 file has 278 of unique tokens\n",
      "149 file has 71 of unique tokens\n",
      "44 file has 696 of unique tokens\n",
      "5 file has 379 of unique tokens\n",
      "73 file has 28 of unique tokens\n",
      "76 file has 329 of unique tokens\n",
      "7 file has 514 of unique tokens\n",
      "65 file has 74 of unique tokens\n",
      "41 file has 262 of unique tokens\n",
      "127 file has 103 of unique tokens\n",
      "19 file has 165 of unique tokens\n",
      "164 file has 152 of unique tokens\n",
      "143 file has 460 of unique tokens\n",
      "56 file has 388 of unique tokens\n",
      "43 file has 493 of unique tokens\n",
      "188 file has 412 of unique tokens\n",
      "74 file has 239 of unique tokens\n",
      "132 file has 580 of unique tokens\n",
      "28 file has 530 of unique tokens\n",
      "120 file has 80 of unique tokens\n",
      "24 file has 81 of unique tokens\n",
      "144 file has 121 of unique tokens\n",
      "30 file has 81 of unique tokens\n",
      "67 file has 436 of unique tokens\n",
      "8 file has 368 of unique tokens\n",
      "137 file has 160 of unique tokens\n",
      "156 file has 0 of unique tokens\n",
      "115 file has 70 of unique tokens\n",
      "130 file has 338 of unique tokens\n",
      "114 file has 81 of unique tokens\n",
      "154 file has 559 of unique tokens\n",
      "182 file has 298 of unique tokens\n",
      "84 file has 66 of unique tokens\n",
      "126 file has 111 of unique tokens\n",
      "52 file has 260 of unique tokens\n",
      "177 file has 100 of unique tokens\n",
      "59 file has 256 of unique tokens\n",
      "153 file has 422 of unique tokens\n",
      "60 file has 67 of unique tokens\n",
      "39 file has 258 of unique tokens\n",
      "9 file has 327 of unique tokens\n",
      "101 file has 156 of unique tokens\n",
      "121 file has 2718 of unique tokens\n",
      "161 file has 335 of unique tokens\n",
      "38 file has 190 of unique tokens\n",
      "138 file has 101 of unique tokens\n",
      "27 file has 387 of unique tokens\n",
      "90 file has 110 of unique tokens\n",
      "81 file has 260 of unique tokens\n",
      "155 file has 525 of unique tokens\n",
      "40 file has 548 of unique tokens\n",
      "55 file has 68 of unique tokens\n",
      "6 file has 417 of unique tokens\n",
      "31 file has 697 of unique tokens\n",
      "185 file has 347 of unique tokens\n",
      "2 file has 133 of unique tokens\n",
      "79 file has 338 of unique tokens\n",
      "131 file has 165 of unique tokens\n",
      "48 file has 560 of unique tokens\n",
      "180 file has 331 of unique tokens\n",
      "134 file has 112 of unique tokens\n",
      "181 file has 422 of unique tokens\n",
      "\n",
      "IDF map contains 22176 entries\n",
      "Files processed = 195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "create_tf_idf_for_tokens(lang = \"RU\", webs_pl=ru_webs_pl, input_tokens_file=RU_TOKENS_FILE, output_dir=\"./ru-tokens/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fd6fa66-c845-4f77-abe7-d029c6ecc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF for LEMMAs\n",
    "\n",
    "def create_tf_idf_for_lemmas(lang: str, webs_pl: list, input_lemmas_file: str, output_dir: str):\n",
    "    \n",
    "    total_files_count = len(webs_pl)\n",
    "\n",
    "    \n",
    "    # known lemmas. lemma - key, list of tokens - value\n",
    "    lemmas_map = dict()\n",
    "    file = open(input_lemmas_file, 'r')\n",
    "\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        tokens = re.split('\\\\s+', line)\n",
    "        \n",
    "        lemma_dirty = tokens[0]\n",
    "        lemma = lemma_dirty[:len(lemma_dirty)-1]\n",
    "        \n",
    "        lemmas_map[lemma] = []\n",
    "        for i in range(1, len(tokens) - 1):\n",
    "            token = tokens[i]\n",
    "            if (not len(token.strip()) == 0):\n",
    "                lemmas_map[lemma].append(token)\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "    # token (string) - key, files_appears_in (set of int) - value\n",
    "    idf_files_map = {}\n",
    "    # list of FileTfIdfModel\n",
    "    tfs_per_file_list = []\n",
    "\n",
    "    \n",
    "    for pl in webs_pl:\n",
    "        # list of TfForToken\n",
    "        tfs_list = []\n",
    "    \n",
    "        file_idx_match = re.search(r'(.+)/(\\d+)-.*\\.html$', pl)\n",
    "        file_idx = int(file_idx_match.group(2))\n",
    "        \n",
    "        with open(pl, \"r\") as file:\n",
    "            file_content = file.read()\n",
    "            file_text = h2t.handle(file_content)\n",
    "            \n",
    "            file_tokens = get_tokens(file_text, lang)\n",
    "            file_lemmas = {el.lemma_ for el in file_tokens}\n",
    "    \n",
    "            total_tokens_count = len(file_tokens)\n",
    "            print(f'{file_idx} file has {len(file_lemmas)} lemmas')\n",
    "            \n",
    "            for lemma in file_lemmas:\n",
    "                # сколько раз встречаются токены lemma в тексте данного документа file\n",
    "                tokens_count = 0\n",
    "\n",
    "                tokens_list = lemmas_map[lemma] if (lemma in lemmas_map.keys()) else []\n",
    "                for token in tokens_list:\n",
    "                    t_count = sum(1 for t in file_tokens if t.text == token)\n",
    "                    tokens_count += t_count\n",
    "    \n",
    "                # term frequency для lemma для данного документа\n",
    "                tf = tokens_count / total_tokens_count\n",
    "    \n",
    "                # добавляем lemma в idf_map\n",
    "                if (lemma not in idf_files_map.keys()):\n",
    "                    idf_files_map[lemma] = set()\n",
    "                idf_files_map[lemma].add(file_idx)\n",
    "    \n",
    "                tfs_list.append(TfForToken(lemma, tf))\n",
    "    \n",
    "        file_tfs = FileTfIdfModel(file_idx, tfs_list)\n",
    "        tfs_per_file_list.append(file_tfs)\n",
    "\n",
    "    print()\n",
    "    print(f'IDF map contains {len(idf_files_map.keys())} entries')\n",
    "    print(f'Files processed = {len(tfs_per_file_list)}')\n",
    "\n",
    "    idf_map = calculate_idf_map(idf_files_map=idf_files_map, total_files_count=total_files_count)\n",
    "\n",
    "    write_tf_idf_results(tfs_per_file_list=tfs_per_file_list, idf_map=idf_map, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c941930f-8e66-4f42-8272-73e06377e615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 file has 357 lemmas\n",
      "191 file has 769 lemmas\n",
      "87 file has 178 lemmas\n",
      "174 file has 349 lemmas\n",
      "124 file has 205 lemmas\n",
      "95 file has 89 lemmas\n",
      "169 file has 594 lemmas\n",
      "80 file has 76 lemmas\n",
      "94 file has 76 lemmas\n",
      "49 file has 214 lemmas\n",
      "118 file has 306 lemmas\n",
      "69 file has 163 lemmas\n",
      "194 file has 1087 lemmas\n",
      "139 file has 148 lemmas\n",
      "68 file has 154 lemmas\n",
      "71 file has 38 lemmas\n",
      "89 file has 122 lemmas\n",
      "184 file has 1396 lemmas\n",
      "173 file has 429 lemmas\n",
      "23 file has 161 lemmas\n",
      "51 file has 81 lemmas\n",
      "99 file has 76 lemmas\n",
      "45 file has 66 lemmas\n",
      "128 file has 119 lemmas\n",
      "106 file has 403 lemmas\n",
      "33 file has 529 lemmas\n",
      "136 file has 138 lemmas\n",
      "93 file has 78 lemmas\n",
      "141 file has 133 lemmas\n",
      "13 file has 154 lemmas\n",
      "102 file has 231 lemmas\n",
      "10 file has 517 lemmas\n",
      "116 file has 342 lemmas\n",
      "16 file has 428 lemmas\n",
      "160 file has 482 lemmas\n",
      "183 file has 362 lemmas\n",
      "105 file has 76 lemmas\n",
      "172 file has 349 lemmas\n",
      "112 file has 254 lemmas\n",
      "179 file has 287 lemmas\n",
      "91 file has 132 lemmas\n",
      "58 file has 66 lemmas\n",
      "148 file has 324 lemmas\n",
      "88 file has 117 lemmas\n",
      "178 file has 284 lemmas\n",
      "70 file has 153 lemmas\n",
      "22 file has 265 lemmas\n",
      "190 file has 348 lemmas\n",
      "11 file has 312 lemmas\n",
      "111 file has 402 lemmas\n",
      "109 file has 300 lemmas\n",
      "133 file has 112 lemmas\n",
      "165 file has 408 lemmas\n",
      "162 file has 396 lemmas\n",
      "42 file has 256 lemmas\n",
      "140 file has 128 lemmas\n",
      "187 file has 390 lemmas\n",
      "15 file has 413 lemmas\n",
      "50 file has 224 lemmas\n",
      "75 file has 221 lemmas\n",
      "4 file has 457 lemmas\n",
      "86 file has 446 lemmas\n",
      "170 file has 532 lemmas\n",
      "14 file has 412 lemmas\n",
      "108 file has 355 lemmas\n",
      "166 file has 115 lemmas\n",
      "97 file has 203 lemmas\n",
      "1 file has 331 lemmas\n",
      "142 file has 398 lemmas\n",
      "158 file has 492 lemmas\n",
      "171 file has 671 lemmas\n",
      "21 file has 84 lemmas\n",
      "77 file has 267 lemmas\n",
      "175 file has 613 lemmas\n",
      "122 file has 182 lemmas\n",
      "152 file has 190 lemmas\n",
      "92 file has 66 lemmas\n",
      "35 file has 179 lemmas\n",
      "54 file has 264 lemmas\n",
      "103 file has 424 lemmas\n",
      "110 file has 76 lemmas\n",
      "150 file has 398 lemmas\n",
      "119 file has 368 lemmas\n",
      "36 file has 484 lemmas\n",
      "147 file has 308 lemmas\n",
      "129 file has 94 lemmas\n",
      "163 file has 441 lemmas\n",
      "186 file has 318 lemmas\n",
      "146 file has 76 lemmas\n",
      "96 file has 390 lemmas\n",
      "176 file has 287 lemmas\n",
      "20 file has 267 lemmas\n",
      "66 file has 248 lemmas\n",
      "72 file has 38 lemmas\n",
      "192 file has 375 lemmas\n",
      "61 file has 282 lemmas\n",
      "46 file has 64 lemmas\n",
      "168 file has 160 lemmas\n",
      "17 file has 134 lemmas\n",
      "78 file has 204 lemmas\n",
      "193 file has 414 lemmas\n",
      "104 file has 365 lemmas\n",
      "85 file has 278 lemmas\n",
      "53 file has 351 lemmas\n",
      "123 file has 164 lemmas\n",
      "83 file has 154 lemmas\n",
      "157 file has 92 lemmas\n",
      "12 file has 268 lemmas\n",
      "3 file has 412 lemmas\n",
      "117 file has 228 lemmas\n",
      "167 file has 318 lemmas\n",
      "145 file has 300 lemmas\n",
      "25 file has 704 lemmas\n",
      "189 file has 173 lemmas\n",
      "64 file has 109 lemmas\n",
      "98 file has 459 lemmas\n",
      "100 file has 65 lemmas\n",
      "29 file has 66 lemmas\n",
      "57 file has 283 lemmas\n",
      "135 file has 125 lemmas\n",
      "34 file has 442 lemmas\n",
      "125 file has 300 lemmas\n",
      "151 file has 169 lemmas\n",
      "47 file has 376 lemmas\n",
      "18 file has 255 lemmas\n",
      "63 file has 17 lemmas\n",
      "26 file has 403 lemmas\n",
      "32 file has 380 lemmas\n",
      "62 file has 108 lemmas\n",
      "159 file has 554 lemmas\n",
      "195 file has 77 lemmas\n",
      "107 file has 76 lemmas\n",
      "82 file has 413 lemmas\n",
      "37 file has 244 lemmas\n",
      "149 file has 68 lemmas\n",
      "44 file has 553 lemmas\n",
      "5 file has 329 lemmas\n",
      "73 file has 27 lemmas\n",
      "76 file has 279 lemmas\n",
      "7 file has 420 lemmas\n",
      "65 file has 62 lemmas\n",
      "41 file has 230 lemmas\n",
      "127 file has 97 lemmas\n",
      "19 file has 153 lemmas\n",
      "164 file has 139 lemmas\n",
      "143 file has 382 lemmas\n",
      "56 file has 342 lemmas\n",
      "43 file has 400 lemmas\n",
      "188 file has 374 lemmas\n",
      "74 file has 181 lemmas\n",
      "132 file has 501 lemmas\n",
      "28 file has 446 lemmas\n",
      "120 file has 77 lemmas\n",
      "24 file has 76 lemmas\n",
      "144 file has 111 lemmas\n",
      "30 file has 76 lemmas\n",
      "67 file has 349 lemmas\n",
      "8 file has 312 lemmas\n",
      "137 file has 143 lemmas\n",
      "156 file has 0 lemmas\n",
      "115 file has 67 lemmas\n",
      "130 file has 294 lemmas\n",
      "114 file has 76 lemmas\n",
      "154 file has 478 lemmas\n",
      "182 file has 281 lemmas\n",
      "84 file has 64 lemmas\n",
      "126 file has 103 lemmas\n",
      "52 file has 221 lemmas\n",
      "177 file has 95 lemmas\n",
      "59 file has 220 lemmas\n",
      "153 file has 353 lemmas\n",
      "60 file has 65 lemmas\n",
      "39 file has 228 lemmas\n",
      "9 file has 293 lemmas\n",
      "101 file has 141 lemmas\n",
      "121 file has 2050 lemmas\n",
      "161 file has 286 lemmas\n",
      "38 file has 170 lemmas\n",
      "138 file has 93 lemmas\n",
      "27 file has 326 lemmas\n",
      "90 file has 102 lemmas\n",
      "81 file has 221 lemmas\n",
      "155 file has 429 lemmas\n",
      "40 file has 470 lemmas\n",
      "55 file has 66 lemmas\n",
      "6 file has 350 lemmas\n",
      "31 file has 517 lemmas\n",
      "185 file has 305 lemmas\n",
      "2 file has 127 lemmas\n",
      "79 file has 283 lemmas\n",
      "131 file has 153 lemmas\n",
      "48 file has 434 lemmas\n",
      "180 file has 311 lemmas\n",
      "134 file has 101 lemmas\n",
      "181 file has 368 lemmas\n",
      "\n",
      "IDF map contains 12637 entries\n",
      "Files processed = 195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "create_tf_idf_for_lemmas(lang = \"RU\", webs_pl=ru_webs_pl, input_lemmas_file=RU_LEMMAS_FILE, output_dir=\"./ru-lemmas/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d384d-0653-4ed2-9a03-34dc84bc6219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
